{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb80b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lable_studio标注数据转化成BIO格式\n",
    "\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "def json_to_bio(json_file, bio_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    output_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        text = item['data']['text']\n",
    "        entities = item['annotations'][0]['result']\n",
    "        bio_tags = get_bio_tags(text, entities)\n",
    "        \n",
    "        # Prepare data in the desired format\n",
    "        output_item = {\n",
    "            'id': item['id'],\n",
    "            'text': text.split(),  # Split text into words\n",
    "            'ner_tags': [label2id[label] for label in bio_tags]\n",
    "        }\n",
    "        output_data.append(output_item)\n",
    "    \n",
    "    with open(bio_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def get_bio_tags(text, entities):\n",
    "    words = text.split()\n",
    "    bio_tags = ['O'] * len(words)  # Initialize with 'O' for each word\n",
    "    \n",
    "    for entity in entities:\n",
    "        start = entity['value']['start']\n",
    "        end = entity['value']['end']\n",
    "        entity_type = entity['value']['labels'][0]\n",
    "        \n",
    "        for i in range(start, end):\n",
    "            if i < len(words):\n",
    "                bio_tags[i] = 'B-' + entity_type if i == start else 'I-' + entity_type\n",
    "    \n",
    "    return bio_tags\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    label_list = ['O',\"B-Gender\",\"I-Gender\",\"B-Age\",\"I-Age\", \"B-Conditions\",\"I-Conditions\", \n",
    "                \"B-Intervention\",\"I-Intervention\", \"B-Outcome\", \"I-Outcome\"]\n",
    "    label2id = {label: i for i, label in enumerate(label_list)}\n",
    "    id2label = {i: label for i, label in enumerate(label_list)}\n",
    "    input_from_labelstudio = \"project-1-at-2024-07-04-17-24-6231169e.json\"\n",
    "    output_to_spanbase = 'work_project.json'\n",
    "    json_to_bio(input_from_labelstudio, output_to_spanbase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36151fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ner_datasets = load_dataset('json', data_files='work_project.json')\n",
    "\n",
    "train_test_split = ner_datasets['train'].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "valid_test_split = test_dataset.train_test_split(test_size=0.5, shuffle=True, seed=42)\n",
    "valid_dataset = valid_test_split['train']\n",
    "test_dataset = valid_test_split['test']\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "ner_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "print(ner_datasets)   #查看数据总体描述\n",
    "print(ner_datasets[\"train\"][0])  #查看数据样例 train数据集中的某一个样例\n",
    "print(ner_datasets[\"train\"].features) #查看不同数据集中键的说明\n",
    "#label_list=ner_datasets[\"train\"].features[\"ner_tags\"].feature.names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d49d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "yourdatasets=ner_datasets\n",
    "\n",
    "#1.声明和指定一个分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")  \n",
    "#2.声明函数：处理tokens和ner_tags\n",
    "def process_function(examples):\n",
    "    tokenized_exmaples = tokenizer(examples[\"text\"], \n",
    "                                   max_length=350, \n",
    "                                   truncation=True, \n",
    "                                   is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_exmaples.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "        labels.append(label_ids)\n",
    "    tokenized_exmaples[\"labels\"] = labels\n",
    "    return tokenized_exmaples\n",
    "# 3.处理\n",
    "tokenized_datasets= yourdatasets.map(process_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c900835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于所有的非二分类任务，切记要指定num_labels，否则就会device错误\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\", \n",
    "    num_labels=len(label_list))\n",
    "model.config.num_labels\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "print(seqeval)  #查看使用方法\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def eval_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # 将id转换为原始的字符串类型的标签\n",
    "    true_predictions = [\n",
    "        [label_list[p] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels) \n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels) \n",
    "    ]\n",
    "\n",
    "    result = seqeval.compute(predictions=true_predictions, \n",
    "                             references=true_labels, \n",
    "                             mode=\"strict\", \n",
    "                             scheme=\"IOB2\")\n",
    "\n",
    "    return { \"f1\": result[\"overall_f1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ee420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 GPU 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 优化后的 TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"models_for_ner\",\n",
    "    per_device_train_batch_size=64,  # 根据 GPU 内存调整批量大小\n",
    "    per_device_eval_batch_size=128,   # 根据 GPU 内存调整批量大小\n",
    "    evaluation_strategy=\"epoch\",     # 每个 epoch 评估一次\n",
    "    save_strategy=\"epoch\",           # 每个 epoch 保存一次\n",
    "    metric_for_best_model=\"f1\", \n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=5,              # 训练 1 次\n",
    "    fp16=True,                       # 如果支持，使用混合精度训练\n",
    "    dataloader_num_workers=4         # 增加数据加载线程数\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=eval_metric,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ner_datasets = load_dataset('json', data_files='EBM_NLP_2_Huggingface.json')\n",
    "\n",
    "train_test_split = ner_datasets['train'].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "valid_test_split = test_dataset.train_test_split(test_size=0.5, shuffle=True, seed=42)\n",
    "valid_dataset = valid_test_split['train']\n",
    "test_dataset = valid_test_split['test']\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "ner_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "print(ner_datasets)   #查看数据总体描述\n",
    "print(ner_datasets[\"train\"][0])  #查看数据样例 train数据集中的某一个样例\n",
    "print(ner_datasets[\"train\"].features) #查看不同数据集中键的说明\n",
    "#label_list=ner_datasets[\"train\"].features[\"ner_tags\"].feature.names \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "yourdatasets=ner_datasets\n",
    "\n",
    "#1.声明和指定一个分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")  \n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "#2.声明函数：处理tokens和ner_tags\n",
    "def process_function(examples):\n",
    "    tokenized_exmaples = tokenizer(examples[\"text\"], \n",
    "                                   max_length=350, \n",
    "                                   truncation=True, \n",
    "                                   is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"label\"]):\n",
    "        word_ids = tokenized_exmaples.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "        labels.append(label_ids)\n",
    "    tokenized_exmaples[\"labels\"] = labels\n",
    "    return tokenized_exmaples\n",
    "# 3.处理\n",
    "tokenized_datasets= yourdatasets.map(process_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa2a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
