{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#from instruction import construct_instruction\n",
    "#from process_response import process_response\n",
    "from extract_response import extract_response\n",
    "from unsloth import FastLanguageModel \n",
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "random_seed = 100\n",
    "\n",
    "import re \n",
    "\n",
    "def extract_value(pattern, text):\n",
    "    match = re.search(pattern, text)\n",
    "    return match.group(1).strip() if match else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_aug_generate(aug_dataset, modelname, model_generate_dir,args):\n",
    "    start_time = time.time()\n",
    "    max_attempts=args.get(\"max_attempts\")\n",
    "    max_seq_length=args.get(\"max_seq_length\")\n",
    "    max_new_tokens=args.get(\"max_new_tokens\")\n",
    "\n",
    "    responses_primary = []\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                                                        model_name=modelname,\n",
    "                                                        max_seq_length=max_seq_length,\n",
    "                                                        dtype=None,\n",
    "                                                        load_in_4bit=True\n",
    "                                                    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    \n",
    "    for example in tqdm(aug_dataset, desc=\"Processing examples\"):\n",
    "        instruction =''''''\n",
    "        input_text = example['content']\n",
    "        input_text = '.'.join(input_text.split('.')[:-1]) + '.'\n",
    "        augment=example['aug_content']\n",
    "\n",
    "        prompt_format = \"\"\"\n",
    "                ### Instruction:\n",
    "                {}\n",
    "                ### Input:\n",
    "                @@Input\n",
    "                {}\n",
    "                ### Augument:\n",
    "                * @Augment is for find relate words to determine words in @Input\n",
    "                * It is not allowed to extract words from here\n",
    "                {}\n",
    "                ### Response:\n",
    "                \"\"\"\n",
    "        prompt = prompt_format.format(instruction, input_text,augment) + EOS_TOKEN\n",
    "\n",
    "        max_attempts =max_attempts # 最大尝试次数，防止无限循环\n",
    "        attempts = 0\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        generated_ids = model.generate(\n",
    "                                        inputs['input_ids'],\n",
    "                                        attention_mask=inputs['attention_mask'],\n",
    "                                        max_new_tokens=max_new_tokens,  # 增加生成长度以确保足够的信息\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        pad_token_id=tokenizer.pad_token_id\n",
    "                                    )\n",
    "        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        primary = extract_response(response)\n",
    "        print(extract_response)\n",
    "        response = process_response(primary)\n",
    "        \n",
    "        while all(value == '' for value in response.values()) and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "            generated_ids = model.generate(\n",
    "                                            inputs['input_ids'],\n",
    "                                            attention_mask=inputs['attention_mask'],\n",
    "                                            max_new_tokens=max_new_tokens,  # 增加生成长度以确保足够的信息\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            pad_token_id=tokenizer.pad_token_id\n",
    "                                        )\n",
    "            response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            primary = extract_response(response)\n",
    "            response = process_response(primary)\n",
    "        if attempts >= max_attempts:\n",
    "            print(\"达到最大尝试次数，仍未获得有效响应。\")\n",
    "        responses_primary.append(response)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total time taken: {elapsed_time} seconds\")\n",
    "    \n",
    "    \n",
    "    output_file = model_generate_dir\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(responses_primary, f, indent=4)\n",
    "    print(\"大模型生成的json保存成功!!|!\")\n",
    "    return responses_primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_generate(aug_dataset, modelname, args):\n",
    "    start_time = time.time()\n",
    "    max_attempts = args.get(\"max_attempts\")\n",
    "    max_seq_length = args.get(\"max_seq_length\")\n",
    "    max_new_tokens = args.get(\"max_new_tokens\")\n",
    "\n",
    "    # 移动模型加载到循环外部，避免重复加载\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                                                    model_name=modelname,\n",
    "                                                    max_seq_length=max_seq_length,\n",
    "                                                    dtype=None,\n",
    "                                                    load_in_4bit=True\n",
    "                                                )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    instruction = instruction #### instruction of each datasets\n",
    "\n",
    "    responses_primary = []\n",
    "\n",
    "    for example in tqdm(aug_dataset, desc=\"Processing examples\"):\n",
    "        \n",
    "        input_text = example['content']\n",
    "        #input_text = '.'.join(input_text.split('.')[:-1]) + '.'\n",
    "\n",
    "        prompt_format = \"\"\"\n",
    "                ### Instruction:\n",
    "                {}\n",
    "                ### Input:\n",
    "                @@Input\n",
    "                {}\n",
    "                ### Augument:\n",
    "                * @Augment is for find relate words to determine words in @Input\n",
    "                * It is not allowed to extract words from here\n",
    "                {}\n",
    "                ### Response:{}\n",
    "                \"\"\"\n",
    "        prompt = prompt_format.format(instruction, input_text, \"\", \"\") + EOS_TOKEN\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        \n",
    "        # 优化生成设置\n",
    "        generated_ids = model.generate(\n",
    "                                        inputs['input_ids'],\n",
    "                                        attention_mask=inputs['attention_mask'],\n",
    "                                        max_new_tokens=max_new_tokens,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        pad_token_id=tokenizer.pad_token_id\n",
    "                                    )\n",
    "        \n",
    "        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        primary = extract_response(response)\n",
    "        response = process_response(primary)\n",
    "        responses_primary.append(response)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total time taken: {elapsed_time} seconds\")\n",
    "\n",
    "    return responses_primary  # 增加返回值以便调用该函数时能够得到结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "    \"sample_num\":500,\n",
    "    \"max_attempts\":2,\n",
    "    \"max_seq_length\":2048,\n",
    "   \" max_new_tokens\":200,\n",
    "\n",
    "}\n",
    "fourbit_models = [\n",
    "    #\"unsloth/mistral-7b-v0.3-bnb-4bit\",      \n",
    "    #\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    #\"unsloth/llama-3-8b-bnb-4bit\",           \n",
    "    #\"unsloth/llama-3-8b-Instruct-bnb-4bit\",   \n",
    "]\n",
    "\n",
    "import json\n",
    "aug_data=json.load(open(\"/local/home/sumyao/ysmpubmed/Models/NERmodels/merged_auged_ebm.txt\"))\n",
    "for modelname in fourbit_models:\n",
    "    model_generate_dir=modelname.replace(\"unsloth/\",\"\")+\"_zeroaug_generated.txt\"\n",
    "    responses_primary=zeroshot_generate(aug_dataset=aug_data,modelname=modelname,args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer=FastLanguageModel.from_pretrained(\n",
    "                model_name=\"unsloth/mistral-7b-bnb-4bit\", # or llama-8b\n",
    "                max_seq_length=2048,\n",
    "                dtype=None,\n",
    "                load_in_4bit=True\n",
    "\n",
    ")\n",
    "\n",
    "import json\n",
    "aug_data=json.load(open(\"/local/home/sumyao/ysmpubmed/Models/NERmodels/merged_auged_ebm.txt\"))\n",
    "input_text=aug_data[2].get(\"content\")\n",
    "# 测试\n",
    "instruction = instruction #### instruction of each datasets\n",
    "prompt_format = \"\"\"\n",
    "                ### Instruction:\n",
    "                {}\n",
    "                ### Input:\n",
    "                @@Input\n",
    "                {}\n",
    "                ### Augument:\n",
    "                * @Augment is for find relate words to determine words in @Input\n",
    "                * It is not allowed to extract words from here\n",
    "                {}\n",
    "                ### Response:{}\n",
    "                \"\"\"\n",
    "EOS_TOKEN=tokenizer.eos_token\n",
    "prompt = prompt_format.format(instruction, input_text, \"\", \"\") + EOS_TOKEN\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "# 优化生成设置\n",
    "generated_ids = model.generate(\n",
    "                                inputs['input_ids'],\n",
    "                                attention_mask=inputs['attention_mask'],\n",
    "                                max_new_tokens=1200,\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                pad_token_id=tokenizer.pad_token_id\n",
    "                            )\n",
    "\n",
    "response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "primary = extract_response(response)\n",
    "print(primary)\n",
    "response = process_response(primary)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
