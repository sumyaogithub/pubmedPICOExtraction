{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按照词汇计算相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_sentence(sentence):\n",
    "    intervention_list = sentence.split('|')\n",
    "    cleaned_sentence=''\n",
    "    for intervention in intervention_list:\n",
    "        cleaned = re.sub(r'[()|]', '', intervention).strip()\n",
    "        if ':' in cleaned:\n",
    "            cleaned = cleaned.split(':', 1)[1].strip()  # Get content after the first ':'\n",
    "        cleaned_sentence+=' '+cleaned\n",
    "    return cleaned_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /local/home/sumyao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /local/home/sumyao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patient', 'mild', 'disease', 'with'}\n",
      "{'disease'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'participants': ['patient mild disease with', 'disease'],\n",
       " 'interventions': ['tarenflurbil', 'placebo'],\n",
       " 'outcomes': ['alzheimer total month cognitive baseline score in to assessment scale 18 change subscale disease from',\n",
       "  'alzheimer living study month total daily activity baseline score in to cooperative 18 change disease of from']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 确保你已经下载了WordNet词库\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # 清理干预措施中的不必要字符，并提取有效内容\n",
    "    intervention_list = sentence.split('|')\n",
    "    cleaned_sentence = ''\n",
    "    for intervention in intervention_list:\n",
    "        cleaned = re.sub(r'[()|]', '', intervention).strip()\n",
    "        if ':' in cleaned:\n",
    "            cleaned = cleaned.split(':', 1)[1].strip()  # Get content after the first ':'\n",
    "        cleaned_sentence += ' ' + cleaned\n",
    "    return cleaned_sentence.strip()  # 返回去掉前后空格的结果\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # 词形还原函数\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    return set(lemmatized_words)  # 返回去重的词集\n",
    "\n",
    "def filter_content(content, retrieved):\n",
    "    # 转换内容为小写并去掉标点符号，方便匹配\n",
    "    content_lower = re.sub(r'[^\\w\\s]', '', content.lower())\n",
    "    content_words = lemmatize_text(content_lower)  # 词形还原处理\n",
    "\n",
    "    filtered_participants = []\n",
    "    filtered_interventions = []\n",
    "    filtered_outcomes = []\n",
    "\n",
    "    # 过滤参与者\n",
    "    for participant in retrieved['participants']:\n",
    "        if participant:  # 只处理非空字符串\n",
    "            participant_lower = participant.lower()\n",
    "            lemmatized_participant = lemmatize_text(participant_lower)\n",
    "            if lemmatized_participant.intersection(content_words):  # 检查是否有交集\n",
    "                print(lemmatized_participant.intersection(content_words))\n",
    "                filtered_participants.append(' '.join(lemmatized_participant.intersection(content_words)))\n",
    "\n",
    "    # 处理干预措施\n",
    "    intervention_sentence = retrieved['interventions']\n",
    "    cleaned_interventions = clean_sentence(intervention_sentence).split()  # 清理并拆分干预措施\n",
    "    for intervention in cleaned_interventions:\n",
    "        intervention_lower = intervention.lower()\n",
    "        lemmatized_intervention = lemmatize_text(intervention_lower)\n",
    "        if lemmatized_intervention.intersection(content_words):  # 检查是否有交集\n",
    "            filtered_interventions.append(' '.join(lemmatized_intervention.intersection(content_words)))\n",
    "\n",
    "    # 过滤结果\n",
    "    for outcome in retrieved['outcomes']:\n",
    "        outcome_lower = outcome.lower()\n",
    "        lemmatized_outcome = lemmatize_text(outcome_lower)\n",
    "        if lemmatized_outcome.intersection(content_words):  # 检查是否有交集\n",
    "            filtered_outcomes.append(' '.join(lemmatized_outcome.intersection(content_words)))\n",
    "\n",
    "    return {\n",
    "        'participants': filtered_participants,\n",
    "        'interventions': filtered_interventions,\n",
    "        'outcomes': filtered_outcomes\n",
    "    }\n",
    "\n",
    "# 示例内容和检索到的内容\n",
    "content = \"Title : Effect of tarenflurbil on cognitive decline and activities of daily living in patients with mild Alzheimer disease : a randomized controlled trial . METHODS : A multicenter , randomized , double - blind , placebo - controlled trial enrolling patients with mild AD was conducted at 133 trial sites in the United States between February 21 , 2005 , and April 30 , 2008 . Concomitant treatment with cholinesterase inhibitors or memantine was permitted . Tarenflurbil , 800 mg , or placebo , administered twice a day . Co - primary efficacy end points were the change from baseline to month 18 in total score on the subscale of the Alzheimer Disease Assessment Scale - Cognitive Subscale ( ADAS - Cog , 80 - point version ) and Alzheimer Disease Cooperative Studies - activities of daily living ( ADCS - ADL ) scale . Additional prespecified slope analyses explored the possibility of disease modification . -DOCSTART-\"\n",
    "\n",
    "retrieved = {\n",
    "    'participants': ['', 'patients with mild alzheimers disease', 'alzheimers disease'],\n",
    "    'interventions': 'drug tarenflurbil|drug placebo',\n",
    "    'outcomes': [\n",
    "        'alzheimer disease assessment scale cognitive subscale adascog change in total score from baseline to month 18',\n",
    "        'alzheimer disease cooperative studies activities of daily living adcsadl change in total score from baseline to month 18'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 执行过滤\n",
    "filtered_output = filter_content(content, retrieved)\n",
    "filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "{'alzheimer', 'disease'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'participants': ['alzheimer disease'],\n",
       " 'interventions': ['donepezil', '23', 'mg', 'donepezil', '10', 'mg'],\n",
       " 'outcomes': ['wa the patient week daily 24 scale with change baseline this observation a caregiver to measure state rating interview carried and function using 0 last forward sib in score of from',\n",
       "  'the patient week daily 24 score scale with change baseline a to measure state and function 0 disease in examination of from']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = \"Title  Effectiveness and tolerability of high - dose ( 23 mg / d ) versus standard - dose ( 10 mg / d ) donepezil in moderate to severe Alzheimer  s disease : A 24 - week , randomized , double - blind study . METHODS : This randomized , double - blind study was conducted at 219 sites in Asia , Europe , Australia , North America , South Africa , and South America from June 6 , 2007 , to March 27 , 2009 . Patients aged 45 to 90 years with probable AD , Mini - Mental State Examination score 0 to 20 ( moderate to severe impairment ) , and who were receiving donepezil 10 mg once daily for > or =12 weeks before the start of the study were eligible . Patients ( n = 1467 ) were randomly assigned to receive high - dose donepezil ( 23 mg once daily ) or standard - dose donepezil ( 10 mg once daily ) for 24 weeks . Coprimary effectiveness measures were changes in cognition and global functioning , as assessed using least squares mean changes from baseline ( LSM [ SE ] A ) scores ( last observation carried forward ) on the Severe Impairment Battery ( SIB ; cognition ) and the Clinician s Interview - Based Impression of Change Plus Caregiver Input scale ( CIBIC + ; global function rating ) overall change score ( mean [ SD ] ) at week 24 . Treatment - emergent adverse events ( TEAEs ) were assessed using spontaneous patient / caregiver reporting and open - ended questioning ; clinical laboratory testing ( hematology , biochemistry , and urinalysis panels analyzed by a central laboratory ) ; 12 - lead ECG ; and physical and neurologic examinations , including vital sign measurements . -DOCSTART-\"\n",
    "print(type(contents))\n",
    "retrieved =  {\n",
    "            \"participants\": [\n",
    "                \"\",\n",
    "                \"ALL\",\n",
    "                \"Alzheimer's Disease\"\n",
    "            ],\n",
    "            \"interventions\": \"DRUG: Aricept (donepezil SR 23 mg)|DRUG: Aricept (donepezil IR 10 mg)\",\n",
    "            \"outcomes\": [\n",
    "                \"Change From Baseline to Week 24 in SIB Total Score, The SIB is an assessment of cognitive dysfunction across nine domains such as memory, language, and orientation. The score ranges from 0 (worst) to 100 (best). This outcome was calculated using the LOCF (last observation carried forward) method., Baseline and Week 24|Overall Change From Baseline in Modified CIBIC+ to Week 24, The CIBIC+ is a rating scale derived from an interview with the patient and caregiver with an independent rater designed to measure several domains of patient function, such as mental/cognitive state, behavior, and activities of daily living. The scores range from 1 (marked improvement) to 7 (marked worsening)., Baseline and Week 24\",\n",
    "                \"Change From Baseline to Week 24 in ADCS-ADL Total Score, The ADCS-ADL (Alzhemier's Disease Cooperative Study-Activities of Daily Living) is a 19-item assessment scale used to measure a patient's basic functional abilities, such as walking, grooming, and bathing.Scores range from 0 to 54, with a higher score indicating greater functional ability., Baseline and Week 24|Change From Baseline to Week 24 in MMSE Total Score, The MMSE (Mini-Mental State Examination) is a 30-item test that evaluates 5 domains of cognitive function (orientation to time and place, immediate and delayed recall, attention, calculation, and language). The scores range from 0 (most impaired) to 30 (no impaiment)., Baseline and Week 24\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# 执行过滤\n",
    "filtered_output = filter_content(contents, retrieved)\n",
    "filtered_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子_TF-IDF计算相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title : Effect of tarenflurbil on cognitive decline and activities of daily living in patients with mild Alzheimer disease : a randomized controlled trial': {'participants': None, 'interventions': None, 'outcomes': 'alzheimer disease cooperative studies activities of daily living adcsadl change in total score from baseline to month 18'}, 'METHODS : A multicenter, randomized, double-blind, placebo-controlled trial enrolling patients with mild AD was conducted at 133 trial sites in the United States between February 21, 2005, and April 30, 2008': {'participants': 'patients with mild alzheimers disease', 'interventions': None, 'outcomes': None}, 'Concomitant treatment with cholinesterase inhibitors or memantine was permitted': {'participants': 'patients with mild alzheimers disease', 'interventions': None, 'outcomes': None}, 'Tarenflurbil, 800 mg, or placebo, administered twice a day': {'participants': None, 'interventions': 'tarenflurbil', 'outcomes': None}, 'Co-primary efficacy end points were the change from baseline to month 18 in total score on the subscale of the Alzheimer Disease Assessment Scale - Cognitive Subscale (ADAS-Cog, 80-point version) and Alzheimer Disease Cooperative Studies - activities of daily living (ADCS-ADL) scale': {'participants': None, 'interventions': None, 'outcomes': 'alzheimer disease assessment scale cognitive subscale adascog change in total score from baseline to month 18'}, 'Additional prespecified slope analyses explored the possibility of disease modification.': {'participants': 'alzheimers disease', 'interventions': None, 'outcomes': None}}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    intervention_list = sentence.split('|')\n",
    "    cleaned_sentence = ''\n",
    "    for intervention in intervention_list:\n",
    "        cleaned = re.sub(r'[()|]', '', intervention).strip()\n",
    "        if ':' in cleaned:\n",
    "            cleaned = cleaned.split(':', 1)[1].strip()  # Get content after the first ':'\n",
    "        cleaned_sentence += ' ' + cleaned\n",
    "    return cleaned_sentence.strip()\n",
    "\n",
    "def find_most_similar_sentence(content, retrieved):\n",
    "    # 清理参与者、干预措施和结果\n",
    "    participants = [p for p in retrieved['participants'] if p]  # 去掉空字符串\n",
    "    interventions = clean_sentence(retrieved['interventions']).split()\n",
    "    outcomes = retrieved['outcomes']\n",
    "\n",
    "    # 将所有句子合并为列表\n",
    "    all_sentences = content.split('. ')\n",
    "    sentences = [s.strip() for s in all_sentences if s]  # 去掉空句子\n",
    "\n",
    "    # 计算TF-IDF矩阵\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences + participants + interventions + outcomes)\n",
    "    \n",
    "    # 计算句子与其他项之间的相似度\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[:len(sentences)], tfidf_matrix[len(sentences):])\n",
    "\n",
    "    # 找到最相似的句子\n",
    "    similar_sentences = {}\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        max_score_index = np.argmax(similarity_scores[idx])  # 找到最大相似度的索引\n",
    "        if max_score_index < len(participants):\n",
    "            similar_sentences[sentence] = {\n",
    "                'participants': participants[max_score_index],\n",
    "                'interventions': None,\n",
    "                'outcomes': None\n",
    "            }\n",
    "        elif max_score_index < len(participants) + len(interventions):\n",
    "            similar_sentences[sentence] = {\n",
    "                'participants': None,\n",
    "                'interventions': interventions[max_score_index - len(participants)],\n",
    "                'outcomes': None\n",
    "            }\n",
    "        else:\n",
    "            similar_sentences[sentence] = {\n",
    "                'participants': None,\n",
    "                'interventions': None,\n",
    "                'outcomes': outcomes[max_score_index - len(participants) - len(interventions)]\n",
    "            }\n",
    "\n",
    "    return similar_sentences\n",
    "\n",
    "# 示例内容和检索到的内容\n",
    "content = \"Title : Effect of tarenflurbil on cognitive decline and activities of daily living in patients with mild Alzheimer disease : a randomized controlled trial. METHODS : A multicenter, randomized, double-blind, placebo-controlled trial enrolling patients with mild AD was conducted at 133 trial sites in the United States between February 21, 2005, and April 30, 2008. Concomitant treatment with cholinesterase inhibitors or memantine was permitted. Tarenflurbil, 800 mg, or placebo, administered twice a day. Co-primary efficacy end points were the change from baseline to month 18 in total score on the subscale of the Alzheimer Disease Assessment Scale - Cognitive Subscale (ADAS-Cog, 80-point version) and Alzheimer Disease Cooperative Studies - activities of daily living (ADCS-ADL) scale. Additional prespecified slope analyses explored the possibility of disease modification.\"\n",
    "\n",
    "retrieved = {\n",
    "    'participants': ['', 'patients with mild alzheimers disease', 'alzheimers disease'],\n",
    "    'interventions': 'drug tarenflurbil|drug placebo',\n",
    "    'outcomes': [\n",
    "        'alzheimer disease assessment scale cognitive subscale adascog change in total score from baseline to month 18',\n",
    "        'alzheimer disease cooperative studies activities of daily living adcsadl change in total score from baseline to month 18'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 执行查找最相似句子\n",
    "most_similar_sentences = find_most_similar_sentence(content, retrieved)\n",
    "print(most_similar_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 加载 BERT 模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    intervention_list = sentence.split('|')\n",
    "    cleaned_sentence = ''\n",
    "    for intervention in intervention_list:\n",
    "        cleaned = re.sub(r'[()|]', '', intervention).strip()\n",
    "        if ':' in cleaned:\n",
    "            cleaned = cleaned.split(':', 1)[1].strip()  # Get content after the first ':'\n",
    "        cleaned_sentence += ' ' + cleaned\n",
    "    return cleaned_sentence.strip()\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # 使用平均池化获取句子嵌入\n",
    "\n",
    "def find_most_similar_sentence(content, retrieved, weights, match_boost=0.3):\n",
    "    # 清理参与者、干预措施和结果\n",
    "    participants = [p for p in retrieved['participants'] if p]  # 去掉空字符串\n",
    "    interventions = clean_sentence(retrieved['interventions']).split()\n",
    "    print(interventions)\n",
    "    outcomes = retrieved['outcomes']\n",
    "\n",
    "    # 将所有句子合并为列表\n",
    "    sentences = content.split('. ')\n",
    "    sentences = [s.strip() for s in sentences if s]  # 去掉空句子\n",
    "\n",
    "    # 计算句子嵌入\n",
    "    sentence_embeddings = [get_sentence_embedding(s) for s in sentences]\n",
    "    participant_embeddings = [get_sentence_embedding(p) for p in participants]\n",
    "    intervention_embeddings = [get_sentence_embedding(i) for i in interventions]\n",
    "    outcome_embeddings = [get_sentence_embedding(o) for o in outcomes]\n",
    "\n",
    "    # 初始化相关结果列表\n",
    "    participants_related, interventions_related, outcomes_related = [], [], []\n",
    "\n",
    "    # 计算相似度，结合包含关键词的优先级\n",
    "    for idx, sentence_embedding in enumerate(sentence_embeddings):\n",
    "        sentence = sentences[idx]\n",
    "\n",
    "        # 计算与参与者的相似度\n",
    "        participant_scores = cosine_similarity([sentence_embedding], participant_embeddings)[0]\n",
    "        max_participant_score = max(participant_scores) * weights['participants']\n",
    "        if any(p.lower() in sentence.lower() for p in participants):  # 检查是否包含参与者关键词\n",
    "            max_participant_score += match_boost  # 提升分数\n",
    "\n",
    "        participants_related.append((sentence, max_participant_score))\n",
    "\n",
    "        # 计算与干预措施的相似度\n",
    "        intervention_scores = cosine_similarity([sentence_embedding], intervention_embeddings)[0]\n",
    "        max_intervention_score = max(intervention_scores) * weights['interventions']\n",
    "        if any(i.lower() in sentence.lower() for i in interventions):  # 检查是否包含干预关键词\n",
    "            max_intervention_score += match_boost  # 提升分数\n",
    "\n",
    "        interventions_related.append((sentence, max_intervention_score))\n",
    "\n",
    "        # 计算与结果的相似度\n",
    "        outcome_scores = cosine_similarity([sentence_embedding], outcome_embeddings)[0]\n",
    "        max_outcome_score = max(outcome_scores) * weights['outcomes']\n",
    "        if any(o.lower() in sentence.lower() for o in outcomes):  # 检查是否包含结果关键词\n",
    "            max_outcome_score += match_boost  # 提升分数\n",
    "        outcomes_related.append((sentence, max_outcome_score))\n",
    "\n",
    "    # 排序并选择最相关的两个句子\n",
    "    participants_related = sorted(participants_related, key=lambda x: x[1], reverse=True)[:2]\n",
    "    interventions_related = sorted(interventions_related, key=lambda x: x[1], reverse=True)[:2]\n",
    "    outcomes_related = sorted(outcomes_related, key=lambda x: x[1], reverse=True)[:2]\n",
    "\n",
    "    return {\n",
    "        'participants_related': participants_related,\n",
    "        'interventions_related': interventions_related,\n",
    "        'outcomes_related': outcomes_related\n",
    "    }\n",
    "\n",
    "# 示例内容和检索到的内容\n",
    "content = \"Title: Effect of tarenflurbil on cognitive decline and activities of daily living in patients with mild Alzheimer disease: a randomized controlled trial. METHODS: A multicenter, randomized, double-blind, placebo-controlled trial enrolling patients with mild AD was conducted at 133 trial sites in the United States between February 21, 2005, and April 30, 2008. Concomitant treatment with cholinesterase inhibitors or memantine was permitted. Tarenflurbil, 800 mg, or placebo, administered twice a day. Co-primary efficacy end points were the change from baseline to month 18 in total score on the subscale of the Alzheimer Disease Assessment Scale - Cognitive Subscale (ADAS-Cog, 80-point version) and Alzheimer Disease Cooperative Studies - activities of daily living (ADCS-ADL) scale. Additional prespecified slope analyses explored the possibility of disease modification.\"\n",
    "\n",
    "retrieved = {\n",
    "    'participants': ['', 'patients with mild alzheimers disease', 'alzheimers disease'],\n",
    "    'interventions': 'drug tarenflurbil|drug placebo',\n",
    "    'outcomes': [\n",
    "        'alzheimer disease assessment scale cognitive subscale adascog change in total score from baseline to month 18',\n",
    "        'alzheimer disease cooperative studies activities of daily living adcsadl change in total score from baseline to month 18'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 定义关键词的权重和匹配提升系数\n",
    "weights = {\n",
    "    'participants': 1.0,\n",
    "    'interventions': 1.0,\n",
    "    'outcomes': 1.0\n",
    "}\n",
    "\n",
    "# 执行查找最相似句子\n",
    "most_similar_sentences = find_most_similar_sentence(content, retrieved, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子_BM25计算相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.声明BM25类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /local/home/sumyao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\"\"\"\n",
    "documents = [\n",
    "    [\"bm25\", \"算法\", \"是\", \"一种\", \"检索\", \"模型\"],\n",
    "    [\"如何\", \"实现\", \"bm25\", \"算法\"],\n",
    "    [\"bm25\", \"在\", \"信息检索\", \"中\", \"非常\", \"重要\"],] # 示例文档\n",
    "bm25 = BM25(documents) # 创建BM25实例\n",
    "query = [\"bm25\", \"算法\"]\n",
    "results = bm25.search(query) # 查询\n",
    "for doc_idx, score in results:\n",
    "    print(f\"文档ID: {doc_idx}, 分数: {score}\")#输出结果\n",
    "\"\"\"\n",
    "class BM25:\n",
    "    def __init__(self, documents, k1=1.5, b=0.75):\n",
    "        \"\"\"初始化BM25类，接受文档列表和两个调节参数\"\"\"\n",
    "        self.documents = documents\n",
    "        self.N = len(documents)  # 文档总数\n",
    "        self.avgdl = sum(len(doc) for doc in documents) / self.N  # 文档平均长度\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.inverted_index = defaultdict(list)  # 倒排索引\n",
    "        self.doc_lengths = []  # 记录每个文档的长度\n",
    "        self.build_index()\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"构建倒排索引并计算每个词项的文档频率和文档长度\"\"\"\n",
    "        for idx, doc in enumerate(self.documents):\n",
    "            self.doc_lengths.append(len(doc))\n",
    "            term_counts = Counter(doc)\n",
    "            for term, freq in term_counts.items():\n",
    "                self.inverted_index[term].append((idx, freq))\n",
    "\n",
    "    def idf(self, term):\n",
    "        \"\"\"计算词项的逆文档频率（IDF）\"\"\"\n",
    "        df = len(self.inverted_index.get(term, []))  # 包含该词的文档数\n",
    "        return math.log((self.N - df + 0.5) / (df + 0.5) + 1)\n",
    "\n",
    "    def score(self, query, doc_idx):\n",
    "        \"\"\"计算查询与指定文档之间的BM25得分\"\"\"\n",
    "        score = 0.0\n",
    "        doc_length = self.doc_lengths[doc_idx]\n",
    "        \n",
    "        # 获取当前文档中所有词项的词频\n",
    "        term_freqs = {term: freq for term, doc_freqs in self.inverted_index.items() \n",
    "                      for doc, freq in doc_freqs if doc == doc_idx}\n",
    "\n",
    "        for term in query:\n",
    "            if term in term_freqs:\n",
    "                f = term_freqs[term]  # 词频\n",
    "                idf = self.idf(term)  # 逆文档频率\n",
    "                numerator = f * (self.k1 + 1)\n",
    "                denominator = f + self.k1 * (1 - self.b + self.b * doc_length / self.avgdl)\n",
    "                score += idf * (numerator / denominator)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def search(self, query, top_n=10):\n",
    "        \"\"\"对查询进行BM25检索并返回相关性最高的前N个文档\"\"\"\n",
    "        scores = {idx: self.score(query, idx) for idx in range(self.N)}\n",
    "        return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子_BM25-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档代号: Doc-1, 分数: 0.7748780250549316\n",
      "['如何', '实现', 'bm25', '算法']\n",
      "文档代号: Doc-0, 分数: 0.6661134958267212\n",
      "['bm25', '算法', '是', '一种', '检索', '模型']\n",
      "文档代号: Doc-2, 分数: 0.41082605719566345\n",
      "['bm25', '在', '信息检索', '中', '非常', '重要']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class BM25_BERT:\n",
    "    def __init__(self, documents, k1=1.5, b=0.75, bert_model=\"bert-base-uncased\"):\n",
    "        self.bm25 = BM25(documents)  # 初始化 BM25\n",
    "        self.documents = documents\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model)  # BERT Tokenizer\n",
    "        self.model = BertModel.from_pretrained(bert_model)  # BERT Model\n",
    "        self.document_embeddings = self.encode_documents()  # 预编码文档\n",
    "\n",
    "    def encode_documents(self):\n",
    "        \"\"\"使用 BERT 对文档进行编码\"\"\"\n",
    "        embeddings = []\n",
    "        for doc in self.documents:\n",
    "            sentence = \" \".join(doc)  # 将文档拼接为单个字符串\n",
    "            inputs = self.tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def encode_query(self, query):\n",
    "        \"\"\"使用 BERT 对查询进行编码\"\"\"\n",
    "        sentence = \" \".join(query)\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    def bert_similarity(self, query_embedding, doc_idx):\n",
    "        \"\"\"计算 BERT 相似度（余弦相似度）\"\"\"\n",
    "        doc_embedding = self.document_embeddings[doc_idx]\n",
    "        cos_sim = np.dot(query_embedding, doc_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding))\n",
    "        return cos_sim\n",
    "\n",
    "    def search(self, query, top_n=10, alpha=0.5):\n",
    "        \"\"\"BM25 和 BERT 集成检索\"\"\"\n",
    "        bm25_scores = {idx: self.bm25.score(query, idx) for idx in range(self.bm25.N)}\n",
    "        query_embedding = self.encode_query(query)\n",
    "        final_scores = {}\n",
    "        for idx, bm25_score in bm25_scores.items():\n",
    "            bert_score = self.bert_similarity(query_embedding, idx)\n",
    "            final_scores[idx] = alpha * bm25_score + (1 - alpha) * bert_score\n",
    "        return sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "# 示例文档\n",
    "documents = [\n",
    "    [\"bm25\", \"算法\", \"是\", \"一种\", \"检索\", \"模型\"],\n",
    "    [\"如何\", \"实现\", \"bm25\", \"算法\"],\n",
    "    [\"bm25\", \"在\", \"信息检索\", \"中\", \"非常\", \"重要\"],\n",
    "]\n",
    "query = [\"bm25\", \"算法\"]\n",
    "\n",
    "# 初始化 BM25 + BERT\n",
    "bm25_bert = BM25_BERT(documents)\n",
    "\n",
    "# 检索\n",
    "results = bm25_bert.search(query)\n",
    "for doc_idx, score in results:\n",
    "    print(f\"文档代号: Doc-{doc_idx}, 分数: {score}\")\n",
    "    print(documents[doc_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json2nlpSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\"\"\"\n",
    "# 示例数据\n",
    "retrieved = {\n",
    "    \"age\": \"\",\n",
    "    \"gender\": \"ALL\",\n",
    "    \"conditions\": \"Alzheimer Disease|Dementia\",\n",
    "    \"interventions\": \"DRUG: MPC-7869|DRUG: MPC-7869\",\n",
    "    \"primary outcome measures\": \"Cognition and activities of daily living, 18 mos\",\n",
    "    \"secondary outcome measures\": \"Global function and behavior, 18 mos\"\n",
    "}\n",
    "# 输出清洗后的句子\n",
    "sentences = create_sentences(retrieved)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "## 输出\n",
    "gender is ALL\n",
    "conditions is Alzheimer Disease\n",
    "conditions is Dementia\n",
    "interventions is MPC-7869\n",
    "interventions is MPC-7869\n",
    "primary outcome measures is Cognition and activities of daily living, 18 mos\n",
    "secondary outcome measures is Global function and behavior, 18 mos\n",
    "\"\"\"\n",
    "\n",
    "def clean_value(value):\n",
    "    if not value:\n",
    "        return []\n",
    "    # 如果包含 '|', 按 '|' 分割\n",
    "    parts = value.split('|')\n",
    "    cleaned_parts = []\n",
    "    # 处理每个部分，只保留 ':' 右边的内容\n",
    "    for part in parts:\n",
    "        cleaned = re.sub(r'[()|]', '', part).strip()  # 去除多余字符\n",
    "        if ':' in cleaned:\n",
    "            cleaned = cleaned.split(':', 1)[1].strip()  # 保留冒号右边的内容\n",
    "        cleaned_parts.append(cleaned)\n",
    "    \n",
    "    return cleaned_parts\n",
    "\n",
    "def create_sentences(retrieved):\n",
    "    sentences = []\n",
    "    for key, value in retrieved.items():\n",
    "        cleaned_values = clean_value(value)\n",
    "        for cleaned_value in cleaned_values:\n",
    "            if cleaned_value:  # 跳过空值\n",
    "                sentences.append(f\"{key} is {cleaned_value}\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.转换json为自然语言+输出相似句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BM25算法检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrived_pio_BM25sentence(content, retrived_label):\n",
    "    content_sentences = sent_tokenize(content) #段落分句子\n",
    "    documents=[item.split() for item in content_sentences] #句子转词汇列表,[[]]\n",
    "    bm25 = BM25(documents)\n",
    "\n",
    "    participants = []\n",
    "    outcomes = []\n",
    "    interventions = []\n",
    "\n",
    "    sentences=create_sentences(retrived_label)\n",
    "    for sentence in sentences:\n",
    "        query = sentence.split()\n",
    "        #print(query)\n",
    "        results = bm25.search(query)\n",
    "        \n",
    "        # clinical transfer and match\n",
    "        for doc_idx, score in results:\n",
    "            if score > 0.9:\n",
    "                sim_document_content = \" \".join(documents[doc_idx])\n",
    "                first_word = query[0].lower()  # 将第一个词汇转为小写以便匹配\n",
    "                if first_word in [\"age\", \"gender\", \"conditions\"]:\n",
    "                    participants.append(sim_document_content)\n",
    "                elif first_word.startswith(\"primary\") or first_word.startswith(\"secondary\"):\n",
    "                    outcomes.append(sim_document_content)\n",
    "                else:\n",
    "                    interventions.append(sim_document_content)\n",
    "    retrieved_participants=\" \".join(participants)\n",
    "    retrieved_outcomes=\" \".join(outcomes)\n",
    "    retrieved_interventions=\" \".join(interventions)\n",
    "    return retrieved_participants,retrieved_interventions,retrieved_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.循环运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-2f96vHBUFQnCImcB7b1dC1C741D04c308a0d5dA91d140b3f\"\n",
    "openai.base_url = \"https://free.gpt.ge/v1/\"\n",
    "openai.default_headers = {\"x-foo\": \"true\"}\n",
    "\n",
    "api_key=\"sk-hZuBOOA4Ohv18QPNxV4OAhx8VE1A32m1LvWeUKpGWl2dMez4\",\n",
    "base_url=\"https://api.chatanywhere.tech/v1\"\n",
    "\n",
    "\n",
    "with open(\"/local/home/sumyao/YSforGIT/dataset/Filtered2Added/sectionspecific_nct_filtered_added_withnocluster.json\",'r') as f:\n",
    "    datasets=json.load(f)\n",
    "\n",
    "predictions=[]\n",
    "#for id in tqdm(range(len(datasets)),desc=\"Processing examples\"):\n",
    "for id in tqdm(range(27),desc=\"Processing examples\"):\n",
    "#for id in tqdm([4,5],desc=\"Processing examples\"):\n",
    "    content,retrived_label=datasets[id].get(\"content\"),datasets[id].get(\"retrieved\")\n",
    "    retrieved_participants,retrieved_interventions,retrieved_outcomes= retrived_pio_BM25sentence(content,retrived_label)  \n",
    "    instruction='''\n",
    "                Please extract the PICO elements. You can refer to the annotation guidelines to understand what words shoud be extracted.\n",
    "                    --**Output in JSON format as follows,make sure do not include the irrealavent values for the key:\n",
    "                    {\n",
    "                        \"participants\": \"\",\n",
    "                        \"interventions\": \"\",\n",
    "                        \"comparator\": \"\",\n",
    "                        \"outcomes\": \"\"\n",
    "                    }\n",
    "        ---**Do not reply with anything beyond this JSON file.'''\n",
    "    retrieved=f'''sentences related to Participants is {retrieved_participants}.\n",
    "                sentences related to Interventions and comparator is{retrieved_interventions}.\n",
    "            sentences related to Outcomes is  {retrieved_outcomes}.\n",
    "            You will be punished if you mismatch the real words mislead by this paragraph'''\n",
    "    retrieved_outcomes=    f'''sentences related to Outcomes is  {retrieved_outcomes}.\n",
    "            You will be punished if you mismatch the real words mislead by this paragraph'''\n",
    "    prompt='''\n",
    "            ### Instruction: {}\n",
    "            ### Input:{}\n",
    "            ### Retrived:{}\n",
    "            ### Response '''.format(instruction,content,retrieved_outcomes,\"\")\n",
    "    #print(prompt)\n",
    "    completion = openai.chat.completions.create(\n",
    "                                        model=\"gpt-3.5-turbo\",\n",
    "                                        messages=[\n",
    "                                            {\n",
    "                                                \"role\": \"user\",\n",
    "                                                \"content\": prompt,\n",
    "                                            },\n",
    "                                        ],\n",
    "                                    )\n",
    "    response=completion.choices[0].message.content\n",
    "    #print(response)\n",
    "    predictions.append(response)\n",
    "\n",
    "with open('/local/home/sumyao/YSforGIT/output/predict_sectionspecific_chatgpt35_outcomesretrieved1.json', 'w') as json_file:\n",
    "    json.dump(predictions, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predict_sectionspecific_chatgpt35_retrieved.json', 'w') as json_file:\n",
    "    json.dump(predictions, json_file,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---评价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from enum import Enum\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"---input:sentence---output: cleaned sentence\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[-/]', ' ', text)\n",
    "    text = re.sub(r'[(),;:-]', '', text)\n",
    "    text = re.sub(r'Not specified', '', text)\n",
    "    text = re.sub(r'not specified', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "class LabelSet(Enum):\n",
    "    SET1 = [\"participants\", \"interventions\", \"comparator\", \"outcomes\"]\n",
    "    SET2=[\"participants\", \"interventions\",  \"outcomes\"]\n",
    "\n",
    "\n",
    "def model_assess(predictions,annotations, labels=LabelSet.SET1.value):\n",
    "    \"\"\"\n",
    "    labels = [\"Age\", \"Gender\", \"Conditions\", \"InterventionName\",\"InterventionProtocol\", \"ComparatorName\", \"Outcomes\", \"OutcomeMeasures\"]\n",
    "    labels = [ \"Age\", \"Gender\", \"Conditions\", \"Intervention\", \"samplesize\", \"Outcomes\"]\n",
    "    labels = [ \"participants\",'interventions','comparator','outcomes']\n",
    "    \"\"\"\n",
    "    # step1 clean text\n",
    "    for prediction in predictions:\n",
    "        for key in prediction:\n",
    "            prediction[key] = preprocess_text(prediction[key])\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        for key in annotation:\n",
    "            annotation[key] = preprocess_text(annotation[key])\n",
    "    \n",
    "    \n",
    "    # step2 calculate mean_token_lenth for each label\n",
    "    average_tokens, total_tokens, count_labels = ({label: 0 for label in labels} for _ in range(3)) #重复生成三个字典，键都是labels中的每个label.对应值都是 0\n",
    "    for anno in annotations:\n",
    "        for label in labels:\n",
    "            anno_tokens = set(anno.get(label, \"\").split())\n",
    "            total_tokens[label] += len(anno_tokens)\n",
    "            if len(anno_tokens) > 0:\n",
    "                count_labels[label] += 1\n",
    "    for label in labels:\n",
    "        if count_labels[label] > 0:\n",
    "            average_tokens[label] = total_tokens[label] / count_labels[label]\n",
    " \n",
    "    # step3: Calculate TP, FP, FN for each label\n",
    "    token_counts = {label: {\"TP\": 0, \"FP\": 0, \"FN\": 0} for label in labels}\n",
    "    for pred, anno in zip(predictions, annotations):\n",
    "        for label in labels:\n",
    "            pred_tokens = set(pred.get(label, \"\").split())\n",
    "            anno_tokens = set(anno.get(label, \"\").split())\n",
    "\n",
    "            if not pred_tokens and not anno_tokens: #均为set() 空集合的时候\n",
    "                TP = average_tokens[label]\n",
    "                #TP=0\n",
    "                FP, FN = 0, 0\n",
    "            elif not pred_tokens and anno_tokens:\n",
    "                TP, FP, FN = 0, 0, len(anno_tokens)\n",
    "            elif pred_tokens and not anno_tokens:\n",
    "                TP, FP, FN = 0, len(pred_tokens), 0\n",
    "            else:\n",
    "                TP = len(pred_tokens.intersection(anno_tokens))\n",
    "                FP = len(pred_tokens - anno_tokens)\n",
    "                FN = len(anno_tokens - pred_tokens)\n",
    "\n",
    "            # Update counters\n",
    "            token_counts[label][\"TP\"] += TP\n",
    "            token_counts[label][\"FP\"] += FP\n",
    "            token_counts[label][\"FN\"] += FN\n",
    "\n",
    "    # Calculate precision, recall, and F1 for each label\n",
    "    metrics = {}\n",
    "    for label in labels:\n",
    "        TP = token_counts[label][\"TP\"]\n",
    "        FP = token_counts[label][\"FP\"]\n",
    "        FN = token_counts[label][\"FN\"]\n",
    "\n",
    "        if TP == 0 and FP == 0 and FN == 0:\n",
    "            precision, recall, f1 = 1, 1, 1\n",
    "        elif TP == 0 and FN > 0:\n",
    "            precision, recall, f1 = 1, 0, 0\n",
    "        elif TP == 0 and FP > 0:\n",
    "            precision, recall, f1 = 0, 0, 0\n",
    "        else:\n",
    "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        metrics[label] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    # Calculate micro-average metrics\n",
    "    total_TP = sum(token_counts[label][\"TP\"] for label in labels)\n",
    "    total_FP = sum(token_counts[label][\"FP\"] for label in labels)\n",
    "    total_FN = sum(token_counts[label][\"FN\"] for label in labels)\n",
    "\n",
    "    micro_precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0\n",
    "    micro_recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0\n",
    "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "\n",
    "    micro_metrics = {\"precision\": micro_precision, \"recall\": micro_recall, \"f1\": micro_f1}\n",
    "\n",
    "    return micro_metrics, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'precision': 0.619144035838121,\n",
       "  'recall': 0.6226107760046831,\n",
       "  'f1': 0.6208725666861248},\n",
       " {'participants': {'precision': 0.9213872832369943,\n",
       "   'recall': 0.5624558927311221,\n",
       "   'f1': 0.6985100788781771},\n",
       "  'interventions': {'precision': 0.30097087378640774,\n",
       "   'recall': 0.4626865671641791,\n",
       "   'f1': 0.36470588235294116},\n",
       "  'comparator': {'precision': 0.46166394779771613,\n",
       "   'recall': 0.5752032520325203,\n",
       "   'f1': 0.5122171945701357},\n",
       "  'outcomes': {'precision': 0.6404761904761904,\n",
       "   'recall': 0.7472222222222222,\n",
       "   'f1': 0.6897435897435897}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 计算rag-pico--zero\n",
    "import os\n",
    "aug_data=json.load(open(\"/local/home/sumyao/YSforGIT/dataset/Filtered2Added/sectionspecific_nct_filtered_added_withnocluster.json\"))\n",
    "annotations=[item['labels'] for item in aug_data][:27]\n",
    "\n",
    "predictions=json.load(open('/local/home/sumyao/YSforGIT/output/predict_sectionspecific_chatgpt35_outcomesretrieved1.json'))\n",
    "\n",
    "micro_metrics, metrics=model_assess(parsed_json_objects,annotations, labels=LabelSet.SET1.value)\n",
    "micro_metrics, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions\n",
    "def parse_json_strings(json_strings):\n",
    "    json_objects = []\n",
    "    for json_string in json_strings:\n",
    "        json_string = re.sub(r'^```json\\n', '', json_string)\n",
    "        json_string = re.sub(r'\\n```$', '', json_string)\n",
    "        json_string = json_string.strip()\n",
    "        try:\n",
    "            # 解析 JSON 字符串\n",
    "            json_object = json.loads(json_string)\n",
    "            json_objects.append(json_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON 解析错误: {e} 在字符串: {json_string}\")\n",
    "    return json_objects\n",
    "\n",
    "# 使用函数解析 JSON 字符串\n",
    "parsed_json_objects = parse_json_strings(predictions)\n",
    "with open('ss_predictions_chatgpt35.json', 'w') as json_file:\n",
    "    json.dump(parsed_json_objects, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---案例分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 第一个案例 纯提示\n",
    "'{\\n    \"participants\": \"patients with mild Alzheimer disease\",\\n   \n",
    " \"interventions\": \"Tarenflurbil, 800 mg, or placebo, administered twice a day\",\\n   \n",
    "  \"comparator\": \"placebo\",\\n    \n",
    "  \"outcomes\": \"change from baseline to month 18 in total score on the subscale of the Alzheimer Disease Assessment Scale - Cognitive Subscale (ADAS-Cog, 80-point version) and Alzheimer Disease Cooperative Studies - activities of daily living (ADCS-ADL) scale\"\\n}'\n",
    "## 第一个案例 只加三句话\n",
    "'{\\n    \"participants\": \"patients with mild Alzheimer disease\",\\n  \n",
    "  \"interventions\": \"Tarenflurbil, 800 mg, or placebo, administered twice a day. Concomitant treatment with cholinesterase inhibitors or memantine was permitted.\",\\n   \n",
    "   \"comparator\": \"\",\\n   \n",
    "    \"outcomes\": \"change from baseline to month 18 in total score on the subscale of the Alzheimer Disease Assessment Scale - Cognitive Subscale (ADAS-Cog, 80-point version) and Alzheimer Disease Cooperative Studies - activities of daily living (ADCS-ADL) scale\"\\n}'\n",
    "## 第一个案例 加上对检索内容的批判\n",
    "'{\\n    \"participants\": \"patients with mild Alzheimer disease\",\\n    \n",
    "\"interventions\": \"Tarenflurbil, 800 mg, or placebo, administered twice a day\",\\n   \n",
    " \"comparator\": \"placebo\",\\n   \n",
    "  \"outcomes\": \"change from baseline to month 18 in total score on the subscale of the Alzheimer Disease Assessment Scale - Cognitive Subscale (ADAS-Cog, 80-point version) and Alzheimer Disease Cooperative Studies - activities of daily living (ADCS-ADL) scale\"\\n}'\n",
    "##第二个案例 纯提示\n",
    "  '{\\n    \"participants\": \"patients aged 45 to 90 years with probable AD, Mini-Mental State Examination score 0 to 20 (moderate to severe impairment), and who were receiving donepezil 10 mg once daily for > or =12 weeks before the start of the study\",\\n    \n",
    "  \"interventions\": \"high-dose donepezil (23 mg once daily) or standard-dose donepezil (10 mg once daily)\",\\n    \n",
    "  \"comparator\": \"standard-dose donepezil\",\\n    \n",
    "  \"outcomes\": \"changes in cognition and global functioning\"\\n}'\n",
    "\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
